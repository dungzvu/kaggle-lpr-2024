{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBA94aXwH0H"
      },
      "source": [
        "# Install deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W1FUZV5wH0I"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "os.environ[\"BACKUP_DIR\"] = \"/content/drive/MyDrive/WIP\"\n",
        "os.environ[\"VERSION\"] = \"01-peft-30epochs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieoB_idcwOWB",
        "outputId": "b06eef4b-da0f-4d07-89ad-ba148d753969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkAj9DNHwH0I",
        "outputId": "e55dd2ee-9164-432f-bc7e-4fa9cf310d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# for auto train\n",
        "!pip install -U autotrain-advanced > install_logs.txt\n",
        "!autotrain setup --colab > setup_logs.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rznuwneiOCm5",
        "outputId": "78ad87c6-6184-45a4-92df-fa5d9ca7ea22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrMLjOEWwH0I"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def sharpen_cosine_similarity(x, y):\n",
        "    A = np.array(x)\n",
        "    B = np.array(y)\n",
        "    dot_product = np.dot(A, B)\n",
        "    magnitude_A = np.linalg.norm(A)\n",
        "    magnitude_B = np.linalg.norm(B)\n",
        "    cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
        "    return cosine_similarity ** 3\n",
        "\n",
        "def calculate_score(predicts, targets):\n",
        "    embedding_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
        "    encoded_predicts = embedding_model.encode(predicts)\n",
        "    encoded_targets = embedding_model.encode(targets)\n",
        "    score = [\n",
        "        sharpen_cosine_similarity(target, predict)\n",
        "        for target, predict in zip(encoded_targets, encoded_predicts)\n",
        "    ]\n",
        "    return score\n",
        "\n",
        "def truncate_sentence(text, max_words):\n",
        "    if not isinstance(text, str):\n",
        "        print(text)\n",
        "    words = text.split(\" \")\n",
        "    if len(words) <= max_words:\n",
        "        return text\n",
        "    return \" \".join(words[:max_words])\n",
        "\n",
        "def generate_gemma_prompt(original_text, rewritten_text, rewrite_prompt=None, max_length_each=500):\n",
        "    instruction_text = 'Generate a rewrite_prompt that effectively transforms the provided original_text into the provided rewritten_text. The rewrite_prompt must be clearly explain how to the original_text is transformed to the rewritten_text, focus on explaining the changes of tone, writting style, publishing, etc. Keep the rewrite_prompt concise, less than 100 words.'\n",
        "\n",
        "    text = f\"\"\"<start_of_turn>user {instruction_text}\n",
        "Here is the given texts:\n",
        "# original_text:\n",
        "{truncate_sentence(original_text, max_length_each)}\n",
        "\n",
        "# rewritten_text:\n",
        "{truncate_sentence(rewritten_text, max_length_each)}\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\"\"\" + \\\n",
        "    (f\"\"\"\\n{truncate_sentence(rewrite_prompt, max_length_each)}<end_of_turn>\"\"\" if rewrite_prompt else '')\n",
        "\n",
        "    return text\n",
        "\n",
        "class GemmaModel:\n",
        "    def __init__(self, model_name, adapter_model_name=None, device=\"cuda\"):\n",
        "        self.device = device\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quantization_config\n",
        "        )\n",
        "        self.model = model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # check peft\n",
        "        adapter_model = PeftModel.from_pretrained(model, adapter_model_name)\n",
        "        self.model = adapter_model\n",
        "\n",
        "    def predict_prompt(self, original_text, rewritten_text, max_new_tokens=300):\n",
        "        prompt = generate_gemma_prompt(original_text, rewritten_text)\n",
        "        prompt_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            prompt_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            # temperature=0.9,\n",
        "            # top_k=1,\n",
        "            # top_p=0.92,\n",
        "            # num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "            output = output.split(\"<start_of_turn>model\", 1)[1].split(\"<end_of_turn>\")[0].replace(\"<eos>\", \"\")\n",
        "            return output\n",
        "        except e as Exception:\n",
        "            print(e)\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lqGPVIwH0J"
      },
      "source": [
        "# 1. Prepare data & train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t45wltlKwZ5U"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/drive/MyDrive/Kaggle/LLM/rewrite_prompt/data/[LPR]_ourgen_1003 /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wenca97rwH0J",
        "outputId": "e8ee4526-c8d3-4b78-cd92-02fd3116138a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3973, 4) (995, 4)\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data\n",
        "DATA_DIR = Path(\"./data\")\n",
        "CKPT_DIR = Path(\"./\")\n",
        "\n",
        "# load data\n",
        "train_df = pd.read_csv(DATA_DIR / \"train_data.csv\")\n",
        "test_df = pd.read_csv(DATA_DIR / \"test_data.csv\")\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "print(train_df.shape, test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XJsmRWYwH0J"
      },
      "outputs": [],
      "source": [
        "# prepare 'text' in gemma format column for autotrain\n",
        "# from models.utils import generate_gemma_prompt\n",
        "\n",
        "def apply_generate_prompt(row):\n",
        "    return generate_gemma_prompt(row[\"original_text\"], row[\"rewritten_text\"], row[\"rewrite_prompt\"])\n",
        "\n",
        "train_df[\"text\"] = train_df.apply(apply_generate_prompt, axis=1)\n",
        "train_df.to_csv(DATA_DIR / \"train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st6UcpohwH0J"
      },
      "source": [
        "2. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-0tbdWawH0J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 30\n",
        "batch_size = 1\n",
        "block_size = 1024\n",
        "trainer = \"sft\"\n",
        "warmup_ratio = 0.1\n",
        "weight_decay = 0.01\n",
        "gradient_accumulation = 4\n",
        "mixed_precision = \"fp16\"\n",
        "peft = True\n",
        "quantization = \"int4\"\n",
        "lora_r = 16\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "\n",
        "PROJECT_NAME = \"gemma-2bit\"\n",
        "os.environ[\"PROJECT_NAME\"] = PROJECT_NAME\n",
        "os.environ[\"MODEL_NAME\"] = \"google/gemma-2b-it\"\n",
        "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
        "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
        "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
        "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
        "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
        "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
        "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
        "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
        "os.environ[\"PEFT\"] = str(peft)\n",
        "os.environ[\"QUANTIZATION\"] = str(quantization)\n",
        "os.environ[\"LORA_R\"] = str(lora_r)\n",
        "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
        "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT4NIoIRwH0J",
        "outputId": "2cf2d819-7265-4f43-edb2-e2074727f3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, add_eos_token=False, block_size=1024, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision='fp16', quantization='int4', model_max_length=1024, trainer='default', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, chat_template=None, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token=None, repo_id=None, push_to_hub=False, model='google/gemma-2b-it', project_name='gemma-2bit', seed=42, epochs=30, gradient_accumulation=4, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='data/', train_split='train', valid_split=None, batch_size=1, func=<function run_llm_command_factory at 0x79432953c700>)\u001b[0m\n",
            "> \u001b[1mINFO    Dataset: gemma-2bit (lm_training)\n",
            "Train data: ['data//train.csv']\n",
            "Valid data: []\n",
            "Column mapping: {'text': 'text', 'rejected_text': 'rejected', 'prompt': 'prompt'}\n",
            "\u001b[0m\n",
            "Saving the dataset (1/1 shards): 100% 3973/3973 [00:00<00:00, 275802.21 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 3973/3973 [00:00<00:00, 288788.62 examples/s]\n",
            "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
            "> \u001b[1mINFO    {\"model\":\"google/gemma-2b-it\",\"project_name\":\"gemma-2bit\",\"data_path\":\"gemma-2bit/autotrain-data\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":1024,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":\"fp16\",\"lr\":0.0002,\"epochs\":30,\"batch_size\":1,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"chat_template\":null,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.05,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"autotrain_prompt\",\"text_column\":\"autotrain_text\",\"rejected_text_column\":\"autotrain_rejected_text\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
            "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'gemma-2bit/training_params.json']\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-03-10 19:11:04\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-03-10 19:11:04\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['Unnamed: 0', 'original_text', 'rewrite_prompt', 'rewritten_text', 'autotrain_text'],\n",
            "    num_rows: 3973\n",
            "})\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-03-10 19:11:04\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "tokenizer_config.json: 100% 2.16k/2.16k [00:00<00:00, 9.36MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 69.8MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 228MB/s]\n",
            "special_tokens_map.json: 100% 888/888 [00:00<00:00, 5.26MB/s]\n",
            "config.json: 100% 627/627 [00:00<00:00, 3.59MB/s]\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "model.safetensors.index.json: 100% 13.5k/13.5k [00:00<00:00, 49.8MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.95G [00:00<00:35, 139MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.95G [00:00<00:24, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.95G [00:00<00:20, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.95G [00:00<00:19, 251MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.95G [00:00<00:18, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.95G [00:00<00:17, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.95G [00:00<00:17, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.95G [00:00<00:16, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 273M/4.95G [00:01<00:16, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.95G [00:01<00:16, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.95G [00:01<00:16, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.95G [00:01<00:16, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.95G [00:01<00:15, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.95G [00:01<00:15, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.95G [00:01<00:15, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.95G [00:01<00:15, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.95G [00:01<00:15, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.95G [00:02<00:15, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.95G [00:02<00:16, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 619M/4.95G [00:02<00:16, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.95G [00:02<00:17, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.95G [00:02<00:17, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.95G [00:02<00:17, 238MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.95G [00:02<00:18, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.95G [00:03<00:18, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.95G [00:03<00:17, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.95G [00:03<00:17, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.95G [00:03<00:16, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.95G [00:03<00:16, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.95G [00:03<00:16, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 965M/4.95G [00:03<00:16, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.95G [00:03<00:15, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.95G [00:04<00:15, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.95G [00:04<00:15, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.95G [00:04<00:15, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.95G [00:04<00:15, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.95G [00:04<00:14, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.95G [00:04<00:14, 261MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.22G/4.95G [00:04<00:13, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.95G [00:04<00:13, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.95G [00:04<00:13, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.31G/4.95G [00:05<00:13, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.95G [00:05<00:12, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.95G [00:05<00:12, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.95G [00:05<00:12, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.95G [00:05<00:12, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.95G [00:05<00:12, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.95G [00:05<00:12, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.95G [00:05<00:12, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.56G/4.95G [00:05<00:11, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.95G [00:06<00:11, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.95G [00:06<00:11, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.66G/4.95G [00:06<00:11, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.95G [00:06<00:11, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.95G [00:06<00:11, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.95G [00:06<00:11, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.95G [00:06<00:11, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.81G/4.95G [00:06<00:11, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.95G [00:06<00:11, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.95G [00:07<00:10, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.91G/4.95G [00:07<00:10, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.95G [00:07<00:10, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.95G [00:07<00:10, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.95G [00:07<00:10, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.95G [00:07<00:10, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.95G [00:07<00:10, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.95G [00:07<00:10, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.95G [00:07<00:09, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.16G/4.95G [00:08<00:09, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.95G [00:08<00:09, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.95G [00:08<00:09, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.25G/4.95G [00:08<00:09, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.95G [00:08<00:09, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.95G [00:08<00:10, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.95G [00:08<00:10, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.95G [00:08<00:10, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.95G [00:09<00:10, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.95G [00:09<00:10, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.95G [00:09<00:10, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.51G/4.95G [00:09<00:10, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.95G [00:09<00:10, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.95G [00:09<00:10, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.60G/4.95G [00:09<00:09, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.95G [00:10<00:09, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.95G [00:10<00:08, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.95G [00:10<00:08, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.95G [00:10<00:08, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.76G/4.95G [00:10<00:08, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.95G [00:10<00:08, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.95G [00:10<00:08, 264MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.85G/4.95G [00:10<00:07, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.95G [00:10<00:07, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.95G [00:11<00:07, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.95G/4.95G [00:11<00:07, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.95G [00:11<00:07, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.95G [00:11<00:06, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.95G [00:11<00:06, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.95G [00:11<00:06, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.10G/4.95G [00:11<00:06, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.95G [00:11<00:06, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.95G [00:11<00:06, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.20G/4.95G [00:12<00:06, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.95G [00:12<00:06, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.95G [00:12<00:09, 170MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.29G/4.95G [00:12<00:09, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.95G [00:12<00:08, 183MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.36G/4.95G [00:12<00:07, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.95G [00:13<00:07, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.95G [00:13<00:07, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.45G/4.95G [00:13<00:06, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.95G [00:13<00:06, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.95G [00:13<00:06, 236MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.54G/4.95G [00:13<00:05, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.95G [00:13<00:05, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.95G [00:14<00:05, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.64G/4.95G [00:14<00:05, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.95G [00:14<00:05, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.70G/4.95G [00:14<00:05, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.95G [00:14<00:04, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.95G [00:14<00:04, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.80G/4.95G [00:14<00:04, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.95G [00:14<00:04, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.95G [00:14<00:03, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.89G/4.95G [00:15<00:03, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.95G [00:15<00:03, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.95G [00:15<00:03, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 3.98G/4.95G [00:15<00:03, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.95G [00:15<00:03, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.05G/4.95G [00:15<00:03, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.95G [00:15<00:03, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.95G [00:15<00:02, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.14G/4.95G [00:15<00:02, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.95G [00:16<00:02, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.95G [00:16<00:02, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.24G/4.95G [00:16<00:02, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.95G [00:16<00:02, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.30G/4.95G [00:16<00:02, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.33G/4.95G [00:16<00:02, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.95G [00:16<00:02, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.39G/4.95G [00:16<00:01, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.95G [00:16<00:01, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.95G [00:17<00:01, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.49G/4.95G [00:17<00:01, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.95G [00:17<00:01, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.95G [00:17<00:01, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.58G/4.95G [00:17<00:01, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.95G [00:17<00:01, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.65G/4.95G [00:17<00:01, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.68G/4.95G [00:18<00:01, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.95G [00:18<00:00, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.74G/4.95G [00:18<00:00, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.95G [00:18<00:00, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.95G [00:18<00:00, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.83G/4.95G [00:18<00:00, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.95G [00:18<00:00, 234MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.90G/4.95G [00:18<00:00, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.95G [00:19<00:00, 259MB/s]\n",
            "Downloading shards:  50% 1/2 [00:19<00:19, 19.25s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/67.1M [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 31.5M/67.1M [00:00<00:00, 311MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 67.1M/67.1M [00:00<00:00, 284MB/s]\n",
            "Downloading shards: 100% 2/2 [00:19<00:00,  9.81s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.89s/it]\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 673kB/s]\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-03-10 19:11:36\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m321\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
            "Running tokenizer on train dataset:   0% 0/3973 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on train dataset: 100% 3973/3973 [00:01<00:00, 2989.44 examples/s]\n",
            "Grouping texts in chunks of 1024 (num_proc=4): 100% 3973/3973 [00:00<00:00, 4863.77 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-03-10 19:11:39\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "  0% 0/9870 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 3.4799, 'grad_norm': 1.3072835206985474, 'learning_rate': 5.3292806484295853e-05, 'epoch': 0.8}\n",
            "  3% 329/9870 [10:16<4:57:17,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.475, 'grad_norm': 1.377310872077942, 'learning_rate': 0.00010658561296859171, 'epoch': 1.6}\n",
            "  7% 658/9870 [20:33<4:47:13,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.307, 'grad_norm': 1.2822372913360596, 'learning_rate': 0.00015987841945288756, 'epoch': 2.4}\n",
            " 10% 987/9870 [30:50<4:37:01,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.1999, 'grad_norm': 1.3011082410812378, 'learning_rate': 0.00019853653045142408, 'epoch': 3.2}\n",
            "{'loss': 2.1427, 'grad_norm': 1.1204251050949097, 'learning_rate': 0.00019261510750872454, 'epoch': 4.0}\n",
            " 13% 1316/9870 [41:07<4:26:28,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.0715, 'grad_norm': 1.0602693557739258, 'learning_rate': 0.000186693684566025, 'epoch': 4.8}\n",
            " 17% 1645/9870 [51:23<4:16:21,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 2.0144, 'grad_norm': 1.3188965320587158, 'learning_rate': 0.00018077226162332546, 'epoch': 5.6}\n",
            " 20% 1974/9870 [1:01:40<4:06:06,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.9653, 'grad_norm': 1.1752454042434692, 'learning_rate': 0.00017485083868062592, 'epoch': 6.4}\n",
            " 23% 2303/9870 [1:11:57<3:56:06,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.9244, 'grad_norm': 1.1865788698196411, 'learning_rate': 0.00016892941573792638, 'epoch': 7.19}\n",
            "{'loss': 1.8762, 'grad_norm': 1.1533836126327515, 'learning_rate': 0.00016300799279522684, 'epoch': 7.99}\n",
            " 27% 2632/9870 [1:22:14<3:45:45,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.8266, 'grad_norm': 1.322077751159668, 'learning_rate': 0.0001570865698525273, 'epoch': 8.79}\n",
            " 30% 2961/9870 [1:32:31<3:35:36,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.7903, 'grad_norm': 1.4460301399230957, 'learning_rate': 0.00015116514690982776, 'epoch': 9.59}\n",
            " 33% 3290/9870 [1:42:48<3:24:50,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.7359, 'grad_norm': 1.3604658842086792, 'learning_rate': 0.00014524372396712822, 'epoch': 10.39}\n",
            " 37% 3619/9870 [1:53:04<3:14:41,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.7244, 'grad_norm': 1.2347594499588013, 'learning_rate': 0.00013932230102442868, 'epoch': 11.19}\n",
            "{'loss': 1.6979, 'grad_norm': 1.5910348892211914, 'learning_rate': 0.00013340087808172914, 'epoch': 11.99}\n",
            " 40% 3948/9870 [2:03:20<3:03:51,  1.86s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.6559, 'grad_norm': 1.3940383195877075, 'learning_rate': 0.00012750197005516155, 'epoch': 12.79}\n",
            " 43% 4277/9870 [2:13:34<2:53:44,  1.86s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.6254, 'grad_norm': 1.4702402353286743, 'learning_rate': 0.00012158054711246201, 'epoch': 13.59}\n",
            " 47% 4606/9870 [2:23:48<2:43:18,  1.86s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.6104, 'grad_norm': 1.5689302682876587, 'learning_rate': 0.00011565912416976248, 'epoch': 14.39}\n",
            " 50% 4935/9870 [2:34:02<2:33:19,  1.86s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.5833, 'grad_norm': 1.5510369539260864, 'learning_rate': 0.00010973770122706293, 'epoch': 15.19}\n",
            "{'loss': 1.5692, 'grad_norm': 1.803097128868103, 'learning_rate': 0.0001038162782843634, 'epoch': 15.99}\n",
            " 53% 5264/9870 [2:44:17<2:23:29,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.5411, 'grad_norm': 1.7313530445098877, 'learning_rate': 9.789485534166387e-05, 'epoch': 16.79}\n",
            " 57% 5593/9870 [2:54:33<2:13:02,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.5216, 'grad_norm': 1.6377887725830078, 'learning_rate': 9.197343239896433e-05, 'epoch': 17.59}\n",
            " 60% 5922/9870 [3:04:48<2:02:40,  1.86s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.5058, 'grad_norm': 1.8217285871505737, 'learning_rate': 8.605200945626479e-05, 'epoch': 18.39}\n",
            " 63% 6251/9870 [3:15:03<1:52:36,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.5008, 'grad_norm': 1.799109697341919, 'learning_rate': 8.013058651356525e-05, 'epoch': 19.19}\n",
            "{'loss': 1.4848, 'grad_norm': 1.7287002801895142, 'learning_rate': 7.420916357086571e-05, 'epoch': 19.98}\n",
            " 67% 6580/9870 [3:25:19<1:42:24,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.4615, 'grad_norm': 1.942962408065796, 'learning_rate': 6.83102555442981e-05, 'epoch': 20.78}\n",
            " 70% 6909/9870 [3:35:34<1:32:07,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.4563, 'grad_norm': 1.9935595989227295, 'learning_rate': 6.238883260159856e-05, 'epoch': 21.58}\n",
            " 73% 7238/9870 [3:45:50<1:22:01,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.4394, 'grad_norm': 2.0577218532562256, 'learning_rate': 5.646740965889903e-05, 'epoch': 22.38}\n",
            " 77% 7567/9870 [3:56:07<1:11:46,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.4349, 'grad_norm': 1.6716424226760864, 'learning_rate': 5.054598671619949e-05, 'epoch': 23.18}\n",
            "{'loss': 1.4292, 'grad_norm': 1.6100939512252808, 'learning_rate': 4.462456377349994e-05, 'epoch': 23.98}\n",
            " 80% 7896/9870 [4:06:23<1:01:33,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.4107, 'grad_norm': 1.6558501720428467, 'learning_rate': 3.87031408308004e-05, 'epoch': 24.78}\n",
            " 83% 8225/9870 [4:16:39<51:12,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.4105, 'grad_norm': 2.246084213256836, 'learning_rate': 3.2781717888100863e-05, 'epoch': 25.58}\n",
            " 87% 8554/9870 [4:26:55<40:56,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.3929, 'grad_norm': 1.906540036201477, 'learning_rate': 2.686029494540133e-05, 'epoch': 26.38}\n",
            " 90% 8883/9870 [4:37:12<30:43,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.3966, 'grad_norm': 1.9726569652557373, 'learning_rate': 2.0938872002701788e-05, 'epoch': 27.18}\n",
            "{'loss': 1.3868, 'grad_norm': 1.9905747175216675, 'learning_rate': 1.5017449060002254e-05, 'epoch': 27.98}\n",
            " 93% 9212/9870 [4:47:27<20:31,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.3823, 'grad_norm': 1.7876330614089966, 'learning_rate': 9.096026117302715e-06, 'epoch': 28.78}\n",
            " 97% 9541/9870 [4:57:43<10:13,  1.87s/it]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 1.3735, 'grad_norm': 1.9646590948104858, 'learning_rate': 3.1746031746031746e-06, 'epoch': 29.58}\n",
            "{'train_runtime': 18480.5705, 'train_samples_per_second': 2.136, 'train_steps_per_second': 0.534, 'train_loss': 1.7194793082781235, 'epoch': 30.0}\n",
            "100% 9870/9870 [5:08:00<00:00,  1.87s/it]\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-03-11 00:19:40\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m521\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        "--model ${MODEL_NAME} \\\n",
        "--project-name ${PROJECT_NAME} \\\n",
        "--data-path data/ \\\n",
        "--text-column text \\\n",
        "--lr ${LEARNING_RATE} \\\n",
        "--batch-size ${BATCH_SIZE} \\\n",
        "--epochs ${NUM_EPOCHS} \\\n",
        "--block-size ${BLOCK_SIZE} \\\n",
        "--warmup-ratio ${WARMUP_RATIO} \\\n",
        "--lora-r ${LORA_R} \\\n",
        "--lora-alpha ${LORA_ALPHA} \\\n",
        "--lora-dropout ${LORA_DROPOUT} \\\n",
        "--weight-decay ${WEIGHT_DECAY} \\\n",
        "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
        "--quantization ${QUANTIZATION} \\\n",
        "--peft \\\n",
        "--mixed-precision ${MIXED_PRECISION}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbWy3YUFcX8w"
      },
      "outputs": [],
      "source": [
        "# persist model to gg drive\n",
        "! cp -r ${PROJECT_NAME} ${BACKUP_DIR}/${PROJECT_NAME}-${VERSION}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g19_EM6bF1Jx"
      },
      "outputs": [],
      "source": [
        "! cp -r ${BACKUP_DIR}/${PROJECT_NAME}-${VERSION} ${PROJECT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AMFR4e8wH0J"
      },
      "source": [
        "# 3. Predict data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "87f6cb5d4a154da995b083f97b08c6a8",
            "b783847e52424957b9c199865498960c",
            "9ce4fd0aae694a78a0eae2649581f9e2",
            "b96e57fdccbc4e30a2677b1f7f5c0845",
            "a282bbf610d547c4919434c360ee5d9c",
            "7d7e04d136084c68ab093f2ceccefb51",
            "684f55ec457a4610afd425a6ad49f9ab",
            "a24bea187db540edbf5af4ea64ec5677",
            "b120e8217adc47bb8c85ec53caad3d2a",
            "38cf197dcbdd44b2ae7338b7417e734c",
            "3e19f554fe894de0bf0023ce826a799c"
          ]
        },
        "id": "FX7vdPXzwH0K",
        "outputId": "adfcc3f5-b63e-4a73-d228-b18cea84e0cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f6cb5d4a154da995b083f97b08c6a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nreimagine this song as a folk ballad, with lyrics that are simple yet powerful, yet evocative...'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GemmaModel(\n",
        "    model_name=\"google/gemma-2b-it\",\n",
        "    adapter_model_name = CKPT_DIR / PROJECT_NAME,\n",
        ")\n",
        "\n",
        "# test\n",
        "original = \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\n",
        "rewritten = \"Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\"\n",
        "\n",
        "model.predict_prompt(original, rewritten, max_new_tokens=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfPDbrl7HdR0"
      },
      "source": [
        "## 3.1. Evaluate the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bwCPnKCHEP6",
        "outputId": "8198e277-62ae-4045-9cd2-a77a6899674f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 9/995 [02:32<3:52:39, 14.16s/it]"
          ]
        }
      ],
      "source": [
        "test_predict = []\n",
        "\n",
        "model.model.eval()\n",
        "with torch.no_grad():\n",
        "    for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
        "        try:\n",
        "            prompt = model.predict_prompt(row['original_text'], row['rewritten_text'], max_new_tokens=300)\n",
        "            test_predict.append(prompt)\n",
        "        except:\n",
        "            test_predict.append(\"\")\n",
        "\n",
        "test_df['predict_before'] = test_predict\n",
        "\n",
        "score = calculate_score(\n",
        "    test_df['predict'].to_list(),\n",
        "    test_df['rewrite_prompt'].to_list(),\n",
        ")\n",
        "test_df['score'] = score\n",
        "test_df.to_csv(DATA_DIR / \"test_data.csv\")\n",
        "\n",
        "print('Test score stats: ', stats.describe(np.array(score)))\n",
        "print('Mean SCS score: ', np.mean(np.array(score)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382YEl7lM-lS"
      },
      "outputs": [],
      "source": [
        "# backup\n",
        "! cp -r ./data ${BACKUP_DIR}/data-${VERSION}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38cf197dcbdd44b2ae7338b7417e734c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e19f554fe894de0bf0023ce826a799c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "684f55ec457a4610afd425a6ad49f9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d7e04d136084c68ab093f2ceccefb51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f6cb5d4a154da995b083f97b08c6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b783847e52424957b9c199865498960c",
              "IPY_MODEL_9ce4fd0aae694a78a0eae2649581f9e2",
              "IPY_MODEL_b96e57fdccbc4e30a2677b1f7f5c0845"
            ],
            "layout": "IPY_MODEL_a282bbf610d547c4919434c360ee5d9c"
          }
        },
        "9ce4fd0aae694a78a0eae2649581f9e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a24bea187db540edbf5af4ea64ec5677",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b120e8217adc47bb8c85ec53caad3d2a",
            "value": 2
          }
        },
        "a24bea187db540edbf5af4ea64ec5677": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a282bbf610d547c4919434c360ee5d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b120e8217adc47bb8c85ec53caad3d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b783847e52424957b9c199865498960c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d7e04d136084c68ab093f2ceccefb51",
            "placeholder": "​",
            "style": "IPY_MODEL_684f55ec457a4610afd425a6ad49f9ab",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b96e57fdccbc4e30a2677b1f7f5c0845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38cf197dcbdd44b2ae7338b7417e734c",
            "placeholder": "​",
            "style": "IPY_MODEL_3e19f554fe894de0bf0023ce826a799c",
            "value": " 2/2 [00:03&lt;00:00,  1.32s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}