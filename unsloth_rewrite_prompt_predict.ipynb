{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBA94aXwH0H"
      },
      "source": [
        "# Install deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieoB_idcwOWB",
        "outputId": "42c25b45-4ef2-4497-96cc-1e71f30467d4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install -U torch torchvision torchaudio\n",
        "# !pip3 install -U bitsandbytes\n",
        "!pip3 install -U sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eG7pgWg8OAoF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0W1FUZV5wH0I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT = \"unsplot-mistral7b-lpr\"\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "os.environ[\"BACKUP_DIR\"] = \"/content/drive/MyDrive/WIP\"\n",
        "os.environ[\"VERSION\"] = PROJECT\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"kaggle-lpr\"  # name your W&B project\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"unsloth_mistral-it-7b\"  # log all model checkpoints\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rznuwneiOCm5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install -U -q sentence-transformers\n",
        "# !pip install -U -q wandb\n",
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UYiWzf19U442"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from unsloth import FastLanguageModel\n",
        "# from peft import PeftModel\n",
        "import re\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Utils function\n",
        "t5base_model = None\n",
        "\n",
        "# Function to calculate sharpened cosine similarity\n",
        "def sharpened_cosine_similarity(vec1, vec2, exponent=3):\n",
        "    cosine_similarity = torch.nn.functional.cosine_similarity(vec1, vec2, dim=0)\n",
        "    return cosine_similarity ** exponent\n",
        "\n",
        "#provides similarity scores of a test_phrase against an array of phrases\n",
        "def compare_phrases(test_phrase, phrases):\n",
        "    global t5base_model\n",
        "    t5base_model = SentenceTransformer('sentence-t5-base')\n",
        "    if torch.cuda.is_available():\n",
        "        t5base_model = t5base_model.to(torch.device(\"cuda\"))\n",
        "    model = t5base_model\n",
        "\n",
        "    scores = []\n",
        "    test_embedding = model.encode(test_phrase, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "    for phrase in phrases:\n",
        "        compare_embedding = model.encode(phrase, convert_to_tensor=True, show_progress_bar=False)\n",
        "        score = sharpened_cosine_similarity(test_embedding, compare_embedding).item()\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def truncate_sentence(text, max_words):\n",
        "    if not isinstance(text, str):\n",
        "        print(text)\n",
        "    words = text.split(\" \")\n",
        "    if len(words) <= max_words:\n",
        "        return text\n",
        "    return \" \".join(words[:max_words])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lqGPVIwH0J"
      },
      "source": [
        "# 1. Prepare data & train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wenca97rwH0J",
        "outputId": "0913dc86-7873-427e-89bd-6e75960d71a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3200, 5) (800, 5)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>original_text</th>\n",
              "      <th>rewrite_prompt</th>\n",
              "      <th>rewritten_text</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>555.0</td>\n",
              "      <td>Alex held the razor blade up against his wrist...</td>\n",
              "      <td>Rewrite the story as a romcom / love story</td>\n",
              "      <td>## Butterfly Kisses and Soul Mates\\n\\nAlex sat...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Dear Santa, \\n \\n Thanks for the BB gas gun. I...</td>\n",
              "      <td>Rewrite the story as a heartwarming tale</td>\n",
              "      <td>In the quaint town of Snow Creek, where snowfl...</td>\n",
              "      <td>SCwCSnMXwE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>527.0</td>\n",
              "      <td>`` You can not!'' Sir Adalhard protested as mu...</td>\n",
              "      <td>Rewrite the essay with two robots from the future</td>\n",
              "      <td>You can not!'' Sir Adalhard protested as much ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>This prompt is set in an alternate universe wh...</td>\n",
              "      <td>Rewrite the essay as if it is a science fictio...</td>\n",
              "      <td>In the vast expanse of an alien cosmos, where ...</td>\n",
              "      <td>JfhoNgrBCG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Happy, adjective, feeling pleasure and enjoyme...</td>\n",
              "      <td>Rewrite essay as about an optimistic grocery c...</td>\n",
              "      <td>In the bustling aisles of the grocery store, w...</td>\n",
              "      <td>zhtykeylll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                      original_text  \\\n",
              "0       555.0  Alex held the razor blade up against his wrist...   \n",
              "1         NaN  Dear Santa, \\n \\n Thanks for the BB gas gun. I...   \n",
              "2       527.0  `` You can not!'' Sir Adalhard protested as mu...   \n",
              "3         NaN  This prompt is set in an alternate universe wh...   \n",
              "4         NaN  Happy, adjective, feeling pleasure and enjoyme...   \n",
              "\n",
              "                                      rewrite_prompt  \\\n",
              "0         Rewrite the story as a romcom / love story   \n",
              "1           Rewrite the story as a heartwarming tale   \n",
              "2  Rewrite the essay with two robots from the future   \n",
              "3  Rewrite the essay as if it is a science fictio...   \n",
              "4  Rewrite essay as about an optimistic grocery c...   \n",
              "\n",
              "                                      rewritten_text          id  \n",
              "0  ## Butterfly Kisses and Soul Mates\\n\\nAlex sat...         NaN  \n",
              "1  In the quaint town of Snow Creek, where snowfl...  SCwCSnMXwE  \n",
              "2  You can not!'' Sir Adalhard protested as much ...         NaN  \n",
              "3  In the vast expanse of an alien cosmos, where ...  JfhoNgrBCG  \n",
              "4  In the bustling aisles of the grocery store, w...  zhtykeylll  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# data\n",
        "DATA_DIR = Path(\"/datasets/llm_prompt_recovery/\")\n",
        "CKPT_DIR = Path(\"./\")\n",
        "\n",
        "# load data\n",
        "train_df = pd.read_csv(DATA_DIR / \"train_data.csv\")\n",
        "test_df = pd.read_csv(DATA_DIR / \"test_data.csv\")\n",
        "# train_df.dropna(inplace=True)\n",
        "\n",
        "print(train_df.shape, test_df.shape)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Rewrite this article as if it were a myth being told by ancient storytellers.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "choose_df = test_df[test_df['rewrite_prompt'] == \"Rewrite this article as if it were a myth being told by ancient storytellers.\"]\n",
        "choose_df[\"rewrite_prompt\"][35]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st6UcpohwH0J"
      },
      "source": [
        "2. Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AMFR4e8wH0J"
      },
      "source": [
        "# 3. Predict data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FX7vdPXzwH0K"
      },
      "outputs": [],
      "source": [
        "# # model = GemmaModel(\n",
        "# #     model_name=\"google/gemma-2b-it\",\n",
        "# #     adapter_model_name = CKPT_DIR / PROJECT_NAME,\n",
        "# # )\n",
        "\n",
        "# prmodel = PRModel(\n",
        "#     # model_name = PROJECT,\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     # adapter_model_name = CKPT_DIR / PROJECT_NAME\n",
        "#     prompt_builder = prompt_builder,\n",
        "# )\n",
        "\n",
        "# # test\n",
        "# original = \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\n",
        "# rewritten = \"(Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\"\n",
        "\n",
        "# # FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "# # inputs = tokenizer(\n",
        "# # [\n",
        "# #     MistralInstructPromptBuilder().create_test_row(original, rewritten),\n",
        "# # ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# # outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True)\n",
        "# # outputs = [outputs[0][len(inputs[\"input_ids\"][0]):], ]\n",
        "# # predict = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
        "\n",
        "# # import re\n",
        "# # predict = predict.replace(\"\\\\n\", \"\\n\")\n",
        "# # predict = re.sub(r\"^\\s+|\\s+$\", \"\", predict)\n",
        "# # if predict.startswith(\"**Rewrite Prompt:**\"):\n",
        "# #     predict = predict[len(\"**Rewrite Prompt:**\"):].strip()\n",
        "# # print(predict)\n",
        "\n",
        "# prmodel.eval()\n",
        "# prmodel.predict_prompt(original, rewritten)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
            "   \\\\   /|    GPU: Quadro RTX 5000. Max memory: 15.74 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# sentence similarity model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "t5base_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
        "if torch.cuda.is_available():\n",
        "    t5base_model = t5base_model.to(torch.device(\"cuda\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'- Tone: Surprised\\n- Style: Narrative\\n- Theme: Adventure, Jedi\\n- Inspired: Star Wars.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# starwar\n",
        "passage = \"\"\"As I made my way through the Tatooine desert on my journey to the Jedi Temple to receive my lightsaber and begin my quest to bring balance to the galaxy, I was startled by the sound of humming engines rapidly approaching. I spun around, looking for the source. Suddenly, I felt something lightly bump me on the head, and immediately saw what appeared to be a Jedi Knight flying away. But I had never seen a Jedi Knight like this before. It was smaller, fluffier, and as white as snow. A shiny, maybe? I never found out.\n",
        "\n",
        "Looking down at the ground, I saw what had hit me in the head. I had received a yellow envelope, sealed with red wax. I picked it up and flipped it over. `` Master Skywalker'' it said in spindly script.\n",
        "\n",
        "I looked around. Aside from a few Jedi Knights I saw every day making their way to the Temple, there didn't seem to be anyone who might have had something to do with this. I figured opening a letter could n't do that much harm, so I snapped the seal and read the message within:\n",
        "\n",
        "*To Master Skywalker,*\n",
        "\n",
        "*We are pleased to inform you that you have been accepted at the Jedi Order. Please find enclosed a list of all necessary books and equipment. Knight training begins on 1 September. We await your lightsaber by no later than 31 July.*\n",
        "\n",
        "No sooner had I read the words ``a list of all necessary books and equipment'' than a second page fell out of the envelope. It indeed listed the names of several books by authors with strange names, and included odd items such as lightsabers, plants I'd never heard of, and ancient artifacts. Honestly, the thought of dealing with ancient artifacts struck me as rather morbid, but I was too confused by this letter to be very concerned by it.\"\"\"\n",
        "\n",
        "# sea shanty\n",
        "# passage = \"\"\"(Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\n",
        "# \"\"\"\n",
        "\n",
        "prompt = f\"\"\"Give me tone, style, theme and inspired of following passage. Each should be in 1-2 words. The inspired should be neither actors, movies, writer or none and should be in title case.\n",
        "\n",
        "For example:\n",
        "Passage:\n",
        "\\\"\\\"\\\"The sun was setting over the horizon, casting a warm glow over the fields. The air was filled with the sound of birds chirping and the smell of fresh flowers. It was a peaceful scene, one that made me feel at ease.\\\"\\\"\\\"\n",
        "Your answer:\n",
        "- Tone: Peaceful\n",
        "- Style: Narrative\n",
        "- Theme: Nature\n",
        "- Inspired: None.\n",
        "\n",
        "Your turn:\n",
        "Passage\n",
        "\\\"\\\"\\\"{passage}\\\"\\\"\\\"\n",
        "Your answer:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt,\n",
        "    prompt,\n",
        "], return_tensors = \"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    use_cache = True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "# for i in range(outputs.shape[0]):\n",
        "#     print(outputs[i])\n",
        "outputs = [outputs[0][len(inputs[\"input_ids\"][0]):], ]\n",
        "predict = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
        "\n",
        "predict = predict.replace(\"\\\\n\", \"\\n\")\n",
        "predict = re.sub(r\"^\\s+|\\s+$\", \"\", predict)\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# s=\"- Tone: Surprised\\n- Style: Narrative\\n- Theme: Adventure, Jedi\\n- Inspired: Star Wars\\n\\nI hope this is correct. Let me know if there's anything else I can help you with!\\n\\nBest,\\n[Your Name]\"\n",
        "\n",
        "# categories = {}\n",
        "# category_names = [\"tone\", \"style\", \"theme\", \"inspired\"]\n",
        "# for line in s.split(\"\\n\"):\n",
        "#     if not line:\n",
        "#         continue\n",
        "#     if not line.startswith(\"- \"):\n",
        "#         continue\n",
        "#     line = line[2:]\n",
        "#     key, value = line.split(\":\")\n",
        "#     if key.lower() not in category_names:\n",
        "#         continue\n",
        "#     categories[key.strip().lower()] = value.strip()\n",
        "\n",
        "# categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# # sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "# sentences = [\n",
        "#     f\"{categories['tone']} tone\",\n",
        "#     f\"{categories['style']} style\",\n",
        "#     f\"{categories['theme']} theme\",\n",
        "#     f\"inspired by {categories['inspired']}\",\n",
        "# ]\n",
        "# print(sentences)\n",
        "\n",
        "# model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
        "# embeddings = model.encode(sentences)\n",
        "# print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class PromptRecovery:\n",
        "    category_names = [\"tone\", \"style\", \"theme\", \"inspired\"]\n",
        "    \n",
        "    def __init__(self,\n",
        "                 model=None, tokenizer=None,\n",
        "                 model_name=None,\n",
        "                 prompt_builder=None,\n",
        "                 init_peft=False,\n",
        "                 similarity_model=None,\n",
        "                 device=None):\n",
        "        d = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        if device is None:\n",
        "            device = d\n",
        "        self.device = device\n",
        "        self.prompt_builder = prompt_builder\n",
        "        self.infer_max_new_tokens = 100\n",
        "\n",
        "        max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "        dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "        load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "        self.similarity_model = similarity_model\n",
        "\n",
        "        if model and tokenizer:\n",
        "            self.model = model\n",
        "            self.tokenizer = tokenizer\n",
        "        else:\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "                max_seq_length = max_seq_length,\n",
        "                dtype = dtype,\n",
        "                load_in_4bit = load_in_4bit,\n",
        "                # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "            )\n",
        "\n",
        "            if init_peft:\n",
        "                model = FastLanguageModel.get_peft_model(\n",
        "                    model,\n",
        "                    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "                    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "                    lora_alpha = 16,\n",
        "                    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "                    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "                    use_gradient_checkpointing = True,\n",
        "                    random_state = 3407,\n",
        "                    use_rslora = False,  # We support rank stabilized LoRA\n",
        "                    loftq_config = None, # And LoftQ\n",
        "                )\n",
        "\n",
        "            self.model = model\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "    def eval(self):\n",
        "        FastLanguageModel.for_inference(self.model) # Enable native 2x faster inference\n",
        "\n",
        "    # def __init__(self, model, tokenizer):\n",
        "    #     self.model = model\n",
        "    #     self.tokenizer = tokenizer\n",
        "\n",
        "    def extract_categorized_info(self, passages):\n",
        "        template = \"\"\"Give me tone, style, theme and inspired of following passage. Each should be in 1-2 words. The inspired should be neither actors, movies, writer or none and should be in title case.\n",
        "\n",
        "For example:\n",
        "Passage:\n",
        "\\\"\\\"\\\"The sun was setting over the horizon, casting a warm glow over the fields. The air was filled with the sound of birds chirping and the smell of fresh flowers. It was a peaceful scene, one that made me feel at ease.\\\"\\\"\\\"\n",
        "Your answer:\n",
        "- Tone: Peaceful\n",
        "- Style: Narrative\n",
        "- Theme: Nature\n",
        "- Inspired: None.\n",
        "\n",
        "Your turn:\n",
        "Passage\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "Your answer:\n",
        "\"\"\"\n",
        "        prompts = [\n",
        "            template.format(text=truncate_sentence(passage, 100))\n",
        "            for passage in passages\n",
        "        ]\n",
        "        # prompt = template.format(text=passages)\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            return_tensors = \"pt\",\n",
        "            padding=True,\n",
        "        ).to(self.device)\n",
        "\n",
        "        # print(inputs[\"input_ids\"][1])\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            use_cache = True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        list_categories = []\n",
        "        category_names = self.category_names\n",
        "        for i in range(outputs.size(0)):\n",
        "            o = [outputs[i][len(inputs[\"input_ids\"][i]):], ]\n",
        "            predict = self.tokenizer.batch_decode(o, skip_special_tokens = True)[0]\n",
        "\n",
        "            predict = predict.replace(\"\\\\n\", \"\\n\")\n",
        "            predict = re.sub(r\"^\\s+|\\s+$\", \"\", predict)\n",
        "            \n",
        "            # extract categories\n",
        "            predict = predict.lower()\n",
        "            categories = {}\n",
        "            for line in predict.split(\"\\n\"):\n",
        "                if not line:\n",
        "                    continue\n",
        "                if not line.startswith(\"- \"):\n",
        "                    continue\n",
        "                line = line[2:]\n",
        "                key, value = line.split(\":\")\n",
        "                if key.lower() not in category_names:\n",
        "                    continue\n",
        "                value = value.split('.')[0].strip()\n",
        "                categories[key.strip().lower()] = None if value == 'none' else value\n",
        "            list_categories.append(categories)\n",
        "        return list_categories\n",
        "    \n",
        "    def find_top_k_unsimilar(self, original_list, rewritten_list, k=1):\n",
        "        original_embeddings = t5base_model.encode(original_list, convert_to_tensor=True)\n",
        "        rewritten_embeddings = t5base_model.encode(rewritten_list, convert_to_tensor=True)\n",
        "\n",
        "        cosine_similarity = torch.nn.functional.cosine_similarity(original_embeddings, rewritten_embeddings, dim=1)\n",
        "        # print(cosine_similarity)\n",
        "        ascending_order = torch.argsort(cosine_similarity).tolist()\n",
        "        \n",
        "        for i in ascending_order:\n",
        "            p = rewritten_list[i]\n",
        "            if \"None\" in p or \"none\" in p:\n",
        "                continue\n",
        "            return [p, ]\n",
        "        return None\n",
        "\n",
        "    def get_sentences(self, categories):\n",
        "        sentences = [\n",
        "            f\"{categories['tone']} tone\",\n",
        "            f\"{categories['style']} style\",\n",
        "            f\"{categories['theme']} theme\",\n",
        "            f\"inspired by {categories['inspired']}\",\n",
        "        ]\n",
        "        return sentences\n",
        "\n",
        "    def predict_prompt(self, original_text, rewritten_text):\n",
        "        list_categories = self.extract_categorized_info([original_text, rewritten_text])\n",
        "        original_prompts, rewritten_prompts = [self.get_sentences(cat) for cat in list_categories]\n",
        "        # print(original_prompts)\n",
        "        # print(rewritten_prompts)\n",
        "        predicts = self.find_top_k_unsimilar(original_prompts, rewritten_prompts)\n",
        "        if predicts is not None:\n",
        "            return f\"Rewrite this into {', '.join(predicts)}\"\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "t = torch.tensor([1, 2, 3])\n",
        "torch.argsort(t).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['angry, depressed tone', 'confessional style', 'abuse, trauma theme', 'inspired by wicca']\n",
            "['dark, melancholic tone', 'descriptive, narrative style', 'abuse, neglect, isolation theme', 'inspired by None']\n",
            "tensor([0.9176, 0.7944, 0.9277, 0.7834], device='cuda:0')\n",
            "Rewrite this into descriptive, narrative style\n"
          ]
        }
      ],
      "source": [
        "prmodel = PromptRecovery(\n",
        "    # model_name=\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    # init_peft=True,\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    similarity_model=t5base_model,\n",
        ")\n",
        "\n",
        "list_categories = []\n",
        "for index, row in choose_df.iterrows():\n",
        "    prompt = prmodel.predict_prompt(row['original_text'], row['rewritten_text'])\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# # sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "\n",
        "# def get_sentences(categories):\n",
        "#     sentences = [\n",
        "#         f\"{categories['tone']} tone\",\n",
        "#         f\"{categories['style']} style\",\n",
        "#         f\"{categories['theme']} theme\",\n",
        "#         f\"inspired by {categories['inspired']}\",\n",
        "#     ]\n",
        "#     return sentences\n",
        "\n",
        "# sentences = [get_sentences(c) for c in list_categories]\n",
        "# # print(sentences)\n",
        "\n",
        "# t5base_model = SentenceTransformer('sentence-transformers/sentence-t5-base')\n",
        "# embeddings = [t5base_model.encode(s, convert_to_tensor=True) for s in sentences]\n",
        "# # print(embeddings[0].shape)\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     t5base_model = t5base_model.to(torch.device(\"cuda\"))\n",
        "\n",
        "# cosine_similarity = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=1)\n",
        "# print(cosine_similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfPDbrl7HdR0"
      },
      "source": [
        "## 3.1. Evaluate the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7bwCPnKCHEP6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
        "\n",
        "test_predict = []\n",
        "scores = []\n",
        "\n",
        "model.eval()\n",
        "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
        "    try:\n",
        "        prompt = prmodel.predict_prompt(row['original_text'], row['rewritten_text'])\n",
        "        assert prompt is not None\n",
        "    except:\n",
        "        prompt = \"Improve the following text while maintaining the original meaning.\"\n",
        "    test_predict.append(prompt)\n",
        "    # test_scores = compare_phrases(row['rewrite_prompt'], [prompt, ])\n",
        "    # scores.append(test_scores[0])\n",
        "\n",
        "# test_df['rewrite_prompt'] = test_predict\n",
        "# test_df['score'] = scores\n",
        "# test_df.to_csv(DATA_DIR / \"test_data.csv\", index=False)\n",
        "\n",
        "# print('\\nTest score stats: ', stats.describe(np.array(scores)))\n",
        "# print('\\nMean SCS score: ', np.mean(np.array(scores)))\n",
        "\n",
        "# write submission\n",
        "test_df['rewrite_prompt'] = test_predict\n",
        "test_df = test_df[['id', 'rewrite_prompt']]\n",
        "\n",
        "test_df.to_csv('submission.csv', header=True, index=False)\n",
        "sub = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
        "sub.head()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03333a9a70f84554b5f11e4490479a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a77e76130846ce8b771165ad97f18d",
            "placeholder": "​",
            "style": "IPY_MODEL_7e788bbba0864102ad4d4ac49ff6da18",
            "value": " 3200/3200 [00:03&lt;00:00, 1099.35 examples/s]"
          }
        },
        "229cc36fd7e149179c074beac1c05ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e92d73ea8b54131b3b28e6e8ecb7a77",
              "IPY_MODEL_a1e61850f5f247d0ab7be7094a810dc5",
              "IPY_MODEL_b2b1404b24fa444eb2659caa7b23e5de"
            ],
            "layout": "IPY_MODEL_ff45296d59534ec58ceb575f7e39f49a"
          }
        },
        "30dec4651d0346af9e392ad046d5c8c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "406b2b5a9370425292ff9998a23d9bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e65d58da69d4b21af89afec22c8fe43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68825f73bbcc4727b8c0c932bfdb9f57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb6b4fb9c60402cbbd67c4bc29e8f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7770d1198f0c40ddaae32f36fd4e7bda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7903e8c3d41740f195ed572f5e5bf2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e788bbba0864102ad4d4ac49ff6da18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e92d73ea8b54131b3b28e6e8ecb7a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68825f73bbcc4727b8c0c932bfdb9f57",
            "placeholder": "​",
            "style": "IPY_MODEL_30dec4651d0346af9e392ad046d5c8c9",
            "value": "Map: 100%"
          }
        },
        "9788b0e1560845538632289840f9ca3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99ee65ae8bff466883dbb966bcb55535": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1e61850f5f247d0ab7be7094a810dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ee65ae8bff466883dbb966bcb55535",
            "max": 3200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7903e8c3d41740f195ed572f5e5bf2cd",
            "value": 3200
          }
        },
        "aebb3b43ccc4440eb831be56cfe19e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7770d1198f0c40ddaae32f36fd4e7bda",
            "max": 3200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9788b0e1560845538632289840f9ca3e",
            "value": 3200
          }
        },
        "b2b1404b24fa444eb2659caa7b23e5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f674070c3f0e4f3695c4c887dcfd2c3a",
            "placeholder": "​",
            "style": "IPY_MODEL_6cb6b4fb9c60402cbbd67c4bc29e8f7c",
            "value": " 3200/3200 [00:00&lt;00:00, 11839.37 examples/s]"
          }
        },
        "b6a77e76130846ce8b771165ad97f18d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b57bb8140f49179dfa87129762225e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e65d58da69d4b21af89afec22c8fe43",
            "placeholder": "​",
            "style": "IPY_MODEL_c90d0970929948c7962cc8e792936bb0",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "c90d0970929948c7962cc8e792936bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb1f379fc16e491b9743001bc07abd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6b57bb8140f49179dfa87129762225e",
              "IPY_MODEL_aebb3b43ccc4440eb831be56cfe19e4b",
              "IPY_MODEL_03333a9a70f84554b5f11e4490479a74"
            ],
            "layout": "IPY_MODEL_406b2b5a9370425292ff9998a23d9bb8"
          }
        },
        "f674070c3f0e4f3695c4c887dcfd2c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff45296d59534ec58ceb575f7e39f49a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
