{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"./fixed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives_file = DATA_PATH / \"adjectives.txt\"\n",
    "authors_file = DATA_PATH / \"authors.txt\"\n",
    "books_file = DATA_PATH / \"books.txt\"\n",
    "business_documents_file = DATA_PATH / \"business_documents.txt\"\n",
    "movies_file = DATA_PATH / \"movies.txt\"\n",
    "plays_file = DATA_PATH / \"plays.txt\"\n",
    "writting_style_file = DATA_PATH / \"writting_style.txt\"\n",
    "    \n",
    "def lower_all(texts):\n",
    "    return [text.lower() for text in texts]\n",
    "\n",
    "def strip_all(texts):\n",
    "    return [text.strip() for text in texts]\n",
    "\n",
    "def read_lines(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return strip_all(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ObjectiveType(Enum):\n",
    "    AUTHOR = 0\n",
    "    BOOK = 1\n",
    "    DOC = 2\n",
    "    MOVIE = 3\n",
    "    PLAY = 4\n",
    "    STYLE = 5\n",
    "    ADJECTIVE = 6\n",
    "\n",
    "class ObjectiveElement():\n",
    "    # type = authors, books, bu\n",
    "    def __init__(self, type: ObjectiveType, value: str):\n",
    "        self.links = []\n",
    "\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "\n",
    "    def create_prompt(self):\n",
    "        templ = {\n",
    "            ObjectiveType.AUTHOR: \"Rewrite this text but do it using the writing style of {}\",\n",
    "            ObjectiveType.BOOK: \"Rewrite this text but do it using the tone of {}\",\n",
    "            ObjectiveType.DOC: \"Rewrite this text as if it was a {} document\",\n",
    "            ObjectiveType.MOVIE: \"Rewrite this text but do it using the writing style of the movie {}\",\n",
    "            ObjectiveType.PLAY: \"Rewrite this text but do it using the writing style of the play {}\",\n",
    "            ObjectiveType.STYLE: \"Rewrite this text as if it was written as a {}\",\n",
    "            ObjectiveType.ADJECTIVE: \"Rewrite this text but do it using {} tone\",\n",
    "        }\n",
    "        return templ[self.type].format(self.value)\n",
    "    \n",
    "    def explain_prompt(self):\n",
    "        templ = {\n",
    "            ObjectiveType.AUTHOR: \"Describe the writing style of {} in 40 words\",\n",
    "            ObjectiveType.BOOK: \"Describe the writing style of the book \\\"{}\\\" in 40 words\",\n",
    "            ObjectiveType.DOC: \"Describe what is the \\\"{}\\\" document in 40 words\",\n",
    "            ObjectiveType.MOVIE: \"Describe the tone, style, topic of the movie \\\"{}\\\" in 40 words\",\n",
    "            ObjectiveType.PLAY: \"Describe the tone, style, topic of the play \\\"{}\\\" in 40 words\",\n",
    "            ObjectiveType.STYLE: \"Describe the writting style \\\"{}\\\" in 40 words\",\n",
    "        }\n",
    "        return templ.get(self.type, \"\").format(self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = [ObjectiveElement(ObjectiveType.ADJECTIVE, val) for val in lower_all(read_lines(adjectives_file))]\n",
    "stypes = [ObjectiveElement(ObjectiveType.STYLE, val) for val in lower_all(read_lines(writting_style_file))]\n",
    "\n",
    "def parse_author(lines):\n",
    "    return [line.split(\"(\")[0].strip() for line in lines]\n",
    "\n",
    "authors = [ObjectiveElement(ObjectiveType.AUTHOR, val) for val in parse_author(read_lines(authors_file))]\n",
    "docs = [ObjectiveElement(ObjectiveType.DOC, val) for val in lower_all(read_lines(business_documents_file))]\n",
    "\n",
    "def parse_play(lines):\n",
    "    return [line.split(\"by\")[0].strip().replace('\"', \"\") for line in lines]\n",
    "\n",
    "plays = [ObjectiveElement(ObjectiveType.PLAY, val) for val in parse_play(read_lines(plays_file))]\n",
    "\n",
    "def parse_movie(lines):\n",
    "    return [line.split(\"(\")[0].strip() for line in lines]\n",
    "\n",
    "movies = [ObjectiveElement(ObjectiveType.MOVIE, val) for val in parse_movie(read_lines(movies_file))]\n",
    "\n",
    "def parse_book(lines):\n",
    "    return [line.split(\"by\")[0].strip().replace('\"', \"\") for line in lines]\n",
    "\n",
    "books = [ObjectiveElement(ObjectiveType.BOOK, val) for val in parse_book(read_lines(books_file))]\n",
    "\n",
    "all_elements = adjectives + authors + books + docs + movies + plays + stypes\n",
    "print(all_elements.__len__())\n",
    "print(all_elements[-1].create_prompt(), all_elements[-1].explain_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find text to infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"./data/0401/150_suppl_original_text.csv\")\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "df = df[df['original_text'].apply(lambda x: count_words(x) > 50 and count_words(x) < 100 and \"[Your Name]\" not in x)]\n",
    "all_texts = df['original_text'].tolist()\n",
    "\n",
    "def calculate_diversity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    diversity_score = len(unique_words) / len(words)\n",
    "    return diversity_score\n",
    "\n",
    "diversity_scores = [(text, calculate_diversity(text)) for text in all_texts]\n",
    "\n",
    "# Sort by diversity score\n",
    "diversity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Choose the top N texts\n",
    "N = 6\n",
    "diverse_texts = diversity_scores[:N]\n",
    "\n",
    "jsondata = [diverse_texts[i][0] for i in range(N)]\n",
    "print(json.dumps(jsondata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate core data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core = pd.DataFrame({\n",
    "    \"type\": [a.type.name for a in all_elements],\n",
    "    \"objective\": [a.value for a in all_elements],\n",
    "    \"rewrite_prompt\": [a.create_prompt() for a in all_elements],\n",
    "    \"explain_prompt\": [a.explain_prompt() for a in all_elements],\n",
    "})\n",
    "\n",
    "df_core.to_csv(\"core_objectives.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask gemma to build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes accelerate transformers\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def ask_gemma(input_text, max_new_tokens=100):\n",
    "    input_ids = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=max_new_tokens)\n",
    "    results = [\n",
    "        tokenizer.decode(outputs[i][len(input_ids[i]):], skip_special_tokens=True)\n",
    "        for i in range(len(input_text))\n",
    "    ]\n",
    "    if isinstance(input_text, list):\n",
    "        return results\n",
    "    return results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load data first\n",
    "df_core = pd.read_csv(\"core_objectives.csv\", keep_default_na=False)\n",
    "df_core = df_core.iloc[:1]\n",
    "\n",
    "jsondata = [\n",
    "  \"Georgette, a loving mother, always puts her family's needs first. From waking up early to prepare breakfast to tucking her children into bed at night, she is the heart of their home. Her warm hugs and encouraging words make every day brighter. Georgette's unwavering love and dedication make her an incredible mother.\",\n",
    "  \"Hey Mari! Just wanted to share something funny that happened today. So, I was telling my friends about that crazy party we went to last weekend, and you know how I tend to exaggerate things? Well, I may have added a little embellishment to the story. They were cracking up, but I couldn't help but laugh at myself too. Anyway, hope you're having a great day! Let's catch up soon.\",\n",
    "  \"Jerry, a talented musician, nervously stepped onto the stage. As he began to play, his fingers stumbled, and the melody turned into a jumbled mess. The audience fell silent, disappointment filling the air. But Jerry didn't let this failure define him; he practiced harder and returned to the stage stronger than ever.\",\n",
    "  \"Ladies and gentlemen, thank you for joining us today. I stand before you to shed light on the fascinating puffin. Found in the North Atlantic, these adorable birds are known for their colorful beaks and exceptional diving skills. Let's celebrate the puffin's resilience and conservation efforts. Together, we can ensure a bright future for these magnificent creatures. Thank you.\",\n",
    "  \"Dear Mr. Johnson,\\\\n\\\\nI hope this email finds you well. Attached is the invoice for the recent services provided by Gale's Plumbing. The total amount due is $250, which includes the cost of labor and materials. Please review the invoice and kindly make the payment within 14 days. If you have any questions or concerns, feel free to reach out. Thank you for your business!\\\\n\\\\nBest regards,\\\\nEmily Smith\\\\nGale's Plumbing\",\n",
    "  \"Ladies and gentlemen, thank you for being here today. I want to address a common issue we all face: chafing. Whether it's during exercise or everyday activities, chafing can be uncomfortable and irritating. But fear not! With the help of our new product, \\\"ChafeAway,\\\" you can bid farewell to chafing forever. Say goodbye to discomfort and hello to smooth, irritation-free skin. Try \\\"ChafeAway\\\" today and experience the difference for yourself.\"\n",
    "]\n",
    "\n",
    "ls = []\n",
    "for i, row in tqdm.tqdm(df_core.iterrows(), total=len(df_core)):\n",
    "    explain_prompt = row[\"explain_prompt\"]\n",
    "    # if explain_prompt == float('nan'):\n",
    "    #     explain_prompt = None\n",
    "        \n",
    "    # print(f\"explain {explain_prompt}, {type(explain_prompt)}\")\n",
    "    ls_prompt = [] if not explain_prompt else [ explain_prompt ]\n",
    "    ls_prompt += [\n",
    "        '{}: \"\"\"{}\"\"\"'.format(row[\"rewrite_prompt\"], text) for text in jsondata\n",
    "    ]\n",
    "    # print(ls_prompt)\n",
    "    batch_outputs = ask_gemma(ls_prompt, max_new_tokens=200)\n",
    "    desc, ls_rewritten = (\"\", batch_outputs) if not explain_prompt else (batch_outputs[0], batch_outputs[1:])\n",
    "    \n",
    "    d = {\n",
    "        \"type\": row[\"type\"],\n",
    "        \"objective\": row[\"objective\"],\n",
    "        \"description\": desc,\n",
    "    }\n",
    "    d.update({\n",
    "        f\"rewritten_{i}\": ls_rewritten[i] for i in range(len(ls_rewritten))\n",
    "    })\n",
    "    ls.append(d)\n",
    "\n",
    "df_final = pd.DataFrame(ls)\n",
    "df_final.to_csv(\"final_objectives.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.iloc[0]['rewritten_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "df = pd.read_csv(\"final_objectives.csv\", keep_default_na=False)\n",
    "\n",
    "df = df.drop_duplicates(subset='objective')\n",
    "\n",
    "# df[df['type'] == 'AUTHOR'].head()\n",
    "\n",
    "max_length_view = None\n",
    "\n",
    "\n",
    "def lower_first_char(text):\n",
    "    return text[0].lower() + text[1:]\n",
    "\n",
    "\n",
    "def clean_rewrite(text):\n",
    "    text = text.strip()\n",
    "\n",
    "    # remove \"Sure ...\"\n",
    "    p = r'^\\**((Sure|).*[Hh]ere|[Rr]ewritten).+\\:\\**'\n",
    "    text = re.sub(p, '', text)\n",
    "\n",
    "    ## remove someone's Text/Style\n",
    "    p = r'^\\**.+([Tt]ext|[Ss]tyle|[Vv]ersion|[Rr]ewrite)\\:\\**'\n",
    "    text = re.sub(p, '', text)\n",
    "\n",
    "    text = text.strip()\n",
    "    if text.startswith('\"'):\n",
    "        text = text[1:]\n",
    "    if text.endswith('\"'):\n",
    "        text = text[:-1]\n",
    "\n",
    "    if max_length_view:\n",
    "        text = text[:max_length_view]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_objective(text):\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_description(row):\n",
    "    row_type = row['type']\n",
    "    text = row['description']\n",
    "    if not text:\n",
    "        return row\n",
    "\n",
    "    _prefix = {\n",
    "        # 'AUTHOR': 'The author\\'s writing style is ',\n",
    "        # 'BOOK': 'The book\\'s writing style is ',\n",
    "        'DOC': 'The document is ',\n",
    "        'MOVIE': 'The movie\\'s style is ',\n",
    "        'PLAY': 'The play\\'s style is ',\n",
    "        'STYLE': 'The writing style is ',\n",
    "        'ADJECTIVE': 'The tone is ',\n",
    "    }\n",
    "    prefix = _prefix.get(row['type'], '')\n",
    "\n",
    "    text = str(text).strip()\n",
    "    if text.startswith('or less.\\n\\n'):\n",
    "        text = text[len('or less.\\n\\n'):]\n",
    "\n",
    "    if row_type == 'AUTHOR':\n",
    "        parts = re.split(r\"('s writing style is|'s writing is)\", text)\n",
    "        if len(parts) > 2:\n",
    "            text = parts[2]\n",
    "        text = f\"The author's writing style is {lower_first_char(text)}\"\n",
    "    elif row_type == 'BOOK':\n",
    "        text = re.sub(r'^([A-Z].+)+(\\'s|s\\') \".+\"', '', text)\n",
    "        # print(text)\n",
    "        text = re.sub(r'^(The writing style of|The book) \".+\"', '', text)\n",
    "        text = text.replace(row['objective'], '')\n",
    "        parts = re.split(r\"(is characterized by|writing style is)\", text)\n",
    "        # print(parts)\n",
    "        if len(parts) > 2:\n",
    "            text = text[len(parts[0]):]\n",
    "        # if text.startswith('is ') or text.startswith('has '):\n",
    "        text = f\"The book {lower_first_char(text)}\"\n",
    "    elif row_type == 'DOC':\n",
    "        text = re.sub(r'^([Tt]he|[Aa]|[Aa]n) \"*' + re.escape(row['objective']) + '\"* (document|)', 'This ', text, flags=re.IGNORECASE)\n",
    "    elif row_type == 'MOVIE':\n",
    "        text = re.sub(r'^(The movie|The|)\\s*\"*' + row['objective'] +'\"*', 'This movie', text)\n",
    "        text = re.sub(r'^([A-Z].+)+\\s*is \".+\"', 'This movie is', text)\n",
    "        # fix case\n",
    "        text = text.replace(\"Star Wars: A New Hope\", \"This movie\")\n",
    "    elif row_type == 'PLAY':\n",
    "        text = re.sub(r'^(The play|The|)\\s*\"*' + row['objective'] +'\"*', 'This play', text)\n",
    "    elif row_type == 'STYLE':\n",
    "        text = re.sub(r'^(A |)' + row['objective'] +'( writing|)', 'This style is', text, flags=re.IGNORECASE)\n",
    "    else:\n",
    "        try:\n",
    "            text = prefix + lower_first_char(text)\n",
    "        except:\n",
    "            print(parts)\n",
    "\n",
    "    # Remove redundant spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    if max_length_view:\n",
    "        text = text[:max_length_view]\n",
    "\n",
    "    row['description'] = text\n",
    "    return row\n",
    "\n",
    "df['objective'] = df['objective'].apply(clean_objective)\n",
    "df = df.apply(clean_description, axis=1)\n",
    "for i in range(6):\n",
    "    df[f\"rewritten_{i}\"] = df[f\"rewritten_{i}\"].apply(clean_rewrite)\n",
    "\n",
    "# df[df['objective'] == \"William Shakespeare\"][['type', 'objective', 'description']].head(100)\n",
    "\n",
    "df.to_csv(\"final_objectives_cleaned.csv\", index=False)\n",
    "\n",
    "# df[df['type'] == 'ADJECTIVE'][['type', 'objective', 'description']].head(400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['type'] == 'BOOK'][['type', 'objective', 'description']].head(400)\n",
    "\n",
    "# clean_description(df[df['objective'] == 'The War of the Worlds'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do embed style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "style_model = SentenceTransformer('AnnaWegmann/Style-Embedding')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 834/834 [01:29<00:00,  9.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"final_objectives_cleaned.csv\", keep_default_na=False)\n",
    "\n",
    "ls = []\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    sentences = [\n",
    "        row[f'rewritten_{i}'] for i in range(6)\n",
    "    ]\n",
    "    # print(sentences)\n",
    "    embeddings = model.encode(sentences)\n",
    "    ls.append(embeddings)\n",
    "\n",
    "array = np.array(ls)\n",
    "\n",
    "# Save the array to disk\n",
    "np.save('style_embedding.npy', array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
